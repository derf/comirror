#!/usr/bin/env perl
use strict;
use warnings;
use 5.010;

use WWW::Mechanize;

sub line_to_file {
	my ($line, $file) = @_;
	open(my $fh, '>', $file) or die("Can't open $file for writing: $!\n");
	say {$fh} $line;
	close($fh) or die("Can't close $file: $!\n");
	return;
}

my @mechs;
my @images;
my @unique_images;
my ($image_re, $cache) = (q{}) x 2;
my $length;
my $next_link;

local $| = 1;

if (@ARGV != 3 ) {
	die("Need three URLs to compare (first, second, last but one)\n");
}

print 'Fetching pages';

for my $url (@ARGV) {
	push(@mechs, WWW::Mechanize->new( stackdepth => 0 ));
	$mechs[-1]->get($url);
	print q{.};
}

print "\nComparing images";

for my $i ( 0 .. $#mechs ) {
	for my $image ($mechs[$i]->find_all_images()) {
		push(@{$images[$i]}, $image->url_abs());
		say "$i $images[$i]->[-1]";
	}
	print q{.};
}

print "\n";

for my $link ($mechs[0]->find_all_links()) {
	if ($link->url_abs eq $ARGV[1]) {
		$next_link = $link->text;
	}
}

# A bit fragile so far. We assume that every site is exactly the same, except
# for the actual comic image. For this to work, we need to be sure that we are
# not comparing with a first or last site, because those may be missing a
# next/prev icon and therefore confuse us.

for my $i ( 0 .. $#{$images[1]} ) {
	if ($images[1]->[$i] ne $images[2]->[$i]) {
		push(@unique_images, [$images[1]->[$i], $images[2]->[$i]]);
	}
}

# XKCD has a weird robot detection image. So we just take the first
# @unique_images element for now. Again, this could use more elegance some
# time.

if (length($unique_images[0]->[0]) <= length($unique_images[0]->[1])) {
	$length = length($unique_images[0]->[0]);
}
else {
	$length = length($unique_images[0]->[1]);
}

for my $offset ( 0 .. $length ) {
	my $char1 = substr($unique_images[0]->[0], $offset, 1);
	my $char2 = substr($unique_images[0]->[1], $offset, 1);

	if ($char1 ne $char2) {
		$image_re .= q{.+};
		last;
	}

	$cache .= $char1;

	# Prevent using .../something.+ if we happen to have two images whose
	# names start with the same letter(s). Again, fragile.
	if ($char1 =~ / [^a-zA-Z0-9] /x) {
		$image_re .= $cache;
		$cache = q{};
	}
}

line_to_file($ARGV[0], 'last_uri');
line_to_file($image_re, 'image_re');
line_to_file($next_link, 'next_link');

print "\nimage_re: ${image_re}\n\n";
say "\"next\" link text: ${next_link}";
say "If this is correct, type 'comirror' to start mirroring";

__END__

=head1 NAME

B<comirror-setup> - Set up a directory to be used by B<comirror>

=head1 SYNOPSIS

B<comirror-setup> I<comis urls...>

=head1 DESCRIPTION

B<comirror-setup> takes three URL argumets: The very first page of the web
comic, the second page, and the last but one page.

Based on these arguments, it tries to set up the current working directory so
that you only need to call B<comirror> to mirror the webcomic you were
pointing to.  It does this by comparing the last two URLs to determine a
correct image_re and then creating last_uri with the first URL so that
B<comirror> will start at the right point and download the right images.

=head1 OPTIONS

B<comirror-setup> takes no options yet.

=head1 EXIT STATUS

Non-zero on grave errors, zero otherwise.

=head1 CONFIGURATION

B<comirror-setup> is not configurable.

=head1 DEPENDENCIES

This script requires the perl module WWW::Mechanize.

=head1 BUGS AND LIMITATIONS

As with B<comirror> itself: This script has no brain.
So far, this script is very dumb and fragile.  It's recommended to take a
quick glance at the image_re before actually calling comirror to make sure you
get what you want.

=head1 AUTHOR

Copyright (C) @@year@@ by Daniel Friesel E<lt>derf@chaosdorf.deE<gt>

=head1 LICENSE

  0. You just DO WHAT THE FUCK YOU WANT TO.
